{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "279bac37-9481-464e-902c-3dad4410b417",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Collection loop\u001b[39;00m\n\u001b[32m     71\u001b[39m last_url = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m progress_bar = \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCollecting articles\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     75\u001b[39m     data = fetch_gdelt_articles(params, last_url)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\notebook.py:234\u001b[39m, in \u001b[36mtqdm_notebook.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m unit_scale = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    233\u001b[39m total = \u001b[38;5;28mself\u001b[39m.total * unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[38;5;28mself\u001b[39m.container = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m.container.pbar = proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.displayed = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\notebook.py:108\u001b[39m, in \u001b[36mtqdm_notebook.status_printer\u001b[39m\u001b[34m(_, total, desc, ncols)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[32m    110\u001b[39m     pbar = IProgress(\u001b[38;5;28mmin\u001b[39m=\u001b[32m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m=total)\n",
      "\u001b[31mImportError\u001b[39m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "\"\"\"\n",
    "# Sri Lanka Crop Disease News Extractor (2013-Present)\n",
    "GDELT API Data Collection\n",
    "\"\"\"\n",
    "# %%\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Configuration\n",
    "\"\"\"\n",
    "# %%\n",
    "# API Parameters\n",
    "params = {\n",
    "    'query': '(\"crop disease\" OR \"plant pest\" OR \"agricultural blight\") location:SriLanka',\n",
    "    'format': 'json',\n",
    "    'startdatetime': '20130101',\n",
    "    'enddatetime': '20241231',  # Update to current year\n",
    "    'mode': 'artlist',\n",
    "    'maxrecords': 250  # API limit per request\n",
    "}\n",
    "\n",
    "# Output file\n",
    "output_file = 'sri_lanka_crop_disease_news.csv'\n",
    "existing_urls = set()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Data Collection Function\n",
    "\"\"\"\n",
    "# %%\n",
    "def fetch_gdelt_articles(params, last_url=None):\n",
    "    \"\"\"Fetch articles from GDELT API with pagination support\"\"\"\n",
    "    if last_url:\n",
    "        params['lasturl'] = last_url\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            \"https://api.gdeltproject.org/api/v2/doc/doc\",\n",
    "            params=params,\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API Error: {str(e)}\")\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f\"JSON Decode Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Main Collection Process\n",
    "\"\"\"\n",
    "# %%\n",
    "# Initialize DataFrame if file doesn't exist\n",
    "if os.path.exists(output_file):\n",
    "    df = pd.read_csv(output_file)\n",
    "    existing_urls = set(df['url'].tolist())\n",
    "else:\n",
    "    df = pd.DataFrame(columns=['date', 'title', 'url', 'source', 'language'])\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "# Collection loop\n",
    "last_url = None\n",
    "progress_bar = tqdm(desc=\"Collecting articles\")\n",
    "\n",
    "while True:\n",
    "    data = fetch_gdelt_articles(params, last_url)\n",
    "    if not data or 'articles' not in data or len(data['articles']) == 0:\n",
    "        break\n",
    "        \n",
    "    new_articles = []\n",
    "    for article in data['articles']:\n",
    "        if article['url'] not in existing_urls:\n",
    "            new_articles.append({\n",
    "                'date': article.get('date', ''),\n",
    "                'title': article.get('title', ''),\n",
    "                'url': article.get('url', ''),\n",
    "                'source': article.get('source', ''),\n",
    "                'language': article.get('language', '')\n",
    "            })\n",
    "            existing_urls.add(article['url'])\n",
    "    \n",
    "    if new_articles:\n",
    "        pd.DataFrame(new_articles).to_csv(\n",
    "            output_file, \n",
    "            mode='a', \n",
    "            header=not os.path.exists(output_file), \n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "    last_url = data['articles'][-1]['url']\n",
    "    progress_bar.update(len(data['articles']))\n",
    "    time.sleep(2)  # Respect API rate limits\n",
    "\n",
    "progress_bar.close()\n",
    "print(\"Data collection complete!\")\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Data Verification\n",
    "\"\"\"\n",
    "# %%\n",
    "# Load and verify collected data\n",
    "final_df = pd.read_csv(output_file)\n",
    "print(f\"Total articles collected: {len(final_df)}\")\n",
    "print(\"\\nSample data:\")\n",
    "final_df.head()\n",
    "\n",
    "# %% [markdown]\n",
    "\"\"\"\n",
    "## Optional: Content Scraping\n",
    "\"\"\"\n",
    "# %%\n",
    "def scrape_article_content(url):\n",
    "    \"\"\"Scrape full article content from URL\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Common content selectors\n",
    "        selectors = [\n",
    "            'article', \n",
    "            '.article-content', \n",
    "            '#main-content', \n",
    "            '.post-content'\n",
    "        ]\n",
    "        \n",
    "        for selector in selectors:\n",
    "            content = soup.select_one(selector)\n",
    "            if content:\n",
    "                return ' '.join(p.get_text() for p in content.find_all('p'))\n",
    "        \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Only run if you need full content\n",
    "if False:  # Change to True to enable scraping\n",
    "    tqdm.pandas(desc=\"Scraping content\")\n",
    "    final_df['content'] = final_df['url'].progress_apply(scrape_article_content)\n",
    "    final_df.to_csv('sri_lanka_crop_disease_with_content.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "afbdf8b3-59e0-4a44-8173-1e27528973f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [400]>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "909f16de-dddc-4ee6-9dcb-e3f929f87c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['feed', 'entries'])\n"
     ]
    }
   ],
   "source": [
    "print(search.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94253b63-7eaa-4fe6-95fa-b1d3eff5d6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(search['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b838dee8-eb69-4159-8d1b-249ba3c5a8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ntscraper\n",
      "  Downloading ntscraper-0.4.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\the user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ntscraper) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11 in c:\\users\\the user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ntscraper) (4.13.4)\n",
      "Requirement already satisfied: lxml>=4.9 in c:\\users\\the user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ntscraper) (5.4.0)\n",
      "Requirement already satisfied: tqdm>=4.66 in c:\\users\\the user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ntscraper) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\the user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4>=4.11->ntscraper) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\the user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4>=4.11->ntscraper) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\the user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.28->ntscraper) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\the user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.28->ntscraper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\the user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.28->ntscraper) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\the user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.28->ntscraper) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\the user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.66->ntscraper) (0.4.6)\n",
      "Downloading ntscraper-0.4.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: ntscraper\n",
      "Successfully installed ntscraper-0.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ntscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "600e7799-cb6c-45bc-b62a-85413f276d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntscraper import Nitter\n",
    "\n",
    "# Initialize the scraper\n",
    "scraper = Nitter(log_level=1,skip_instance_check=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0dd12974-a0b3-47fe-b873-c1fcbbf78677",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No instance specified and instance check skipped",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example: Search for tweets related to agriculture in Sri Lanka\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mscraper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magriculture Sri Lanka\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mterm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m results[\u001b[33m'\u001b[39m\u001b[33mtweets\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ntscraper\\nitter.py:926\u001b[39m, in \u001b[36mNitter.get_tweets\u001b[39m\u001b[34m(self, terms, mode, number, since, until, near, language, to, replies, filters, exclude, max_retries, instance)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(terms) == \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    924\u001b[39m     term = terms.strip()\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mterm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43msince\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m        \u001b[49m\u001b[43muntil\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(terms) == \u001b[32m1\u001b[39m:\n\u001b[32m    942\u001b[39m     term = terms[\u001b[32m0\u001b[39m].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ntscraper\\nitter.py:725\u001b[39m, in \u001b[36mNitter._search\u001b[39m\u001b[34m(self, term, mode, number, since, until, near, language, to, replies, filters, exclude, max_retries, instance)\u001b[39m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    723\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid mode. Use \u001b[39m\u001b[33m'\u001b[39m\u001b[33mterm\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhashtag\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, or \u001b[39m\u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m language:\n\u001b[32m    728\u001b[39m     endpoint += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m+lang%3A\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ntscraper\\nitter.py:92\u001b[39m, in \u001b[36mNitter._initialize_session\u001b[39m\u001b[34m(self, instance)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_instance_check:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo instance specified and instance check skipped\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28mself\u001b[39m.instance = \u001b[38;5;28mself\u001b[39m.get_random_instance()\n\u001b[32m     94\u001b[39m     logging.info(\n\u001b[32m     95\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo instance specified, using random instance \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.instance\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: No instance specified and instance check skipped"
     ]
    }
   ],
   "source": [
    "# Example: Search for tweets related to agriculture in Sri Lanka\n",
    "results = scraper.get_tweets(\"agriculture Sri Lanka\", mode=\"term\", number=50)\n",
    "\n",
    "# Print results\n",
    "for tweet in results['tweets']:\n",
    "    print(f\"{tweet['date']} - {tweet['user']['username']}: {tweet['text']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17d6f584-aec2-45ad-bf43-f202954d04b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No instance specified and instance check skipped",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m scraper = Nitter(instances=\u001b[33m\"\u001b[39m\u001b[33mhttps://nitter.poast.org\u001b[39m\u001b[33m\"\u001b[39m, skip_instance_check=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Search for tweets\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m results = \u001b[43mscraper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magriculture Sri Lanka\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mterm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Print result tweets\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m results[\u001b[33m'\u001b[39m\u001b[33mtweets\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ntscraper\\nitter.py:926\u001b[39m, in \u001b[36mNitter.get_tweets\u001b[39m\u001b[34m(self, terms, mode, number, since, until, near, language, to, replies, filters, exclude, max_retries, instance)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(terms) == \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    924\u001b[39m     term = terms.strip()\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mterm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43msince\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m        \u001b[49m\u001b[43muntil\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnear\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(terms) == \u001b[32m1\u001b[39m:\n\u001b[32m    942\u001b[39m     term = terms[\u001b[32m0\u001b[39m].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ntscraper\\nitter.py:725\u001b[39m, in \u001b[36mNitter._search\u001b[39m\u001b[34m(self, term, mode, number, since, until, near, language, to, replies, filters, exclude, max_retries, instance)\u001b[39m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    723\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid mode. Use \u001b[39m\u001b[33m'\u001b[39m\u001b[33mterm\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhashtag\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, or \u001b[39m\u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m language:\n\u001b[32m    728\u001b[39m     endpoint += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m+lang%3A\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ntscraper\\nitter.py:92\u001b[39m, in \u001b[36mNitter._initialize_session\u001b[39m\u001b[34m(self, instance)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_instance_check:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo instance specified and instance check skipped\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28mself\u001b[39m.instance = \u001b[38;5;28mself\u001b[39m.get_random_instance()\n\u001b[32m     94\u001b[39m     logging.info(\n\u001b[32m     95\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo instance specified, using random instance \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.instance\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: No instance specified and instance check skipped"
     ]
    }
   ],
   "source": [
    "from ntscraper import Nitter\n",
    "\n",
    "# Manually specify a known working instance\n",
    "scraper = Nitter(instances=\"https://nitter.poast.org\", skip_instance_check=True)\n",
    "\n",
    "# Search for tweets\n",
    "results = scraper.get_tweets(\"agriculture Sri Lanka\", mode=\"term\", number=50)\n",
    "\n",
    "# Print result tweets\n",
    "for tweet in results['tweets']:\n",
    "    print(f\"{tweet['date']} - @{tweet['user']['username']}: {tweet['text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "65be88f6-7b93-47ab-9e6b-6f682c247cbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Nitter.__init__() got an unexpected keyword argument 'instance'. Did you mean 'instances'?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mntscraper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Nitter\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m scraper = \u001b[43mNitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://nitter.poast.org\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_instance_check\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Nitter.__init__() got an unexpected keyword argument 'instance'. Did you mean 'instances'?"
     ]
    }
   ],
   "source": [
    "from ntscraper import Nitter\n",
    "\n",
    "scraper = Nitter(instance=\"https://nitter.poast.org\", skip_instance_check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "208dc20a-4c50-4526-bd16-aea4b9efc584",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\models.py:963\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[32m    965\u001b[39m     \u001b[38;5;66;03m# Wrong UTF codec detected; usually because it's not UTF-8\u001b[39;00m\n\u001b[32m    966\u001b[39m     \u001b[38;5;66;03m# but some other 8-bit codec.  This is an RFC violation,\u001b[39;00m\n\u001b[32m    967\u001b[39m     \u001b[38;5;66;03m# and the server didn't bother to tell us what codec *was*\u001b[39;00m\n\u001b[32m    968\u001b[39m     \u001b[38;5;66;03m# used.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    343\u001b[39m \n\u001b[32m    344\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      6\u001b[39m params = {\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m(\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcrop disease\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m OR \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mplant pest\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m) location:SriLanka\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33martlist\u001b[39m\u001b[33m'\u001b[39m  \u001b[38;5;66;03m# Returns article list with metadata\u001b[39;00m\n\u001b[32m     12\u001b[39m }\n\u001b[32m     14\u001b[39m response = requests.get(url, params=params)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m articles = \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33marticles\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles:\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle[\u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\models.py:971\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    969\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    970\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson.loads(\u001b[38;5;28mself\u001b[39m.text, **kwargs)\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# API endpoint for document search\n",
    "url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "\n",
    "params = {\n",
    "    'query': '(\"crop disease\" OR \"plant pest\") location:SriLanka',\n",
    "    'format': 'json',\n",
    "    'startdatetime': '20130101',\n",
    "    'enddatetime': '20231231',\n",
    "    'mode': 'artlist'  # Returns article list with metadata\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "articles = response.json()['articles']\n",
    "\n",
    "for article in articles:\n",
    "    print(f\"{article['date']} | {article['title']} | {article['url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3589df76-22f1-4cc3-9152-a5c201d024fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\models.py:963\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[32m    965\u001b[39m     \u001b[38;5;66;03m# Wrong UTF codec detected; usually because it's not UTF-8\u001b[39;00m\n\u001b[32m    966\u001b[39m     \u001b[38;5;66;03m# but some other 8-bit codec.  This is an RFC violation,\u001b[39;00m\n\u001b[32m    967\u001b[39m     \u001b[38;5;66;03m# and the server didn't bother to tell us what codec *was*\u001b[39;00m\n\u001b[32m    968\u001b[39m     \u001b[38;5;66;03m# used.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    343\u001b[39m \n\u001b[32m    344\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Send request\u001b[39;00m\n\u001b[32m     17\u001b[39m response = requests.get(url, params=params)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m data = \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Extract data\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m data.get(\u001b[33m'\u001b[39m\u001b[33marticles\u001b[39m\u001b[33m'\u001b[39m, []):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\models.py:971\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    969\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    970\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson.loads(\u001b[38;5;28mself\u001b[39m.text, **kwargs)\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# API Endpoint\n",
    "url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    'query': '(crop disease OR plant pest) location:SriLanka',\n",
    "    'format': 'json',  # Can also use 'csv'\n",
    "    'startdatetime': '20130101',  # From Jan 1, 2013\n",
    "    'enddatetime': '20241231',    # To Dec 31, 2024\n",
    "    'mode': 'artlist',  # Returns article metadata\n",
    "    'maxrecords': 100   # Limit results (API allows up to 250 per call)\n",
    "}\n",
    "\n",
    "# Send request\n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# Extract data\n",
    "for article in data.get('articles', []):\n",
    "    print(f\"Date: {article['date']}\")\n",
    "    print(f\"Headline: {article['title']}\")\n",
    "    print(f\"URL: {article['url']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8bf0c09c-fb1a-4c07-aa15-d74cc2c5c849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode JSON. Raw response: Invalid query start date format (must be in YYYYMMDDHHMMSS format).\n",
      "...\n",
      "No data received\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "\n",
    "def get_gdelt_articles(query, start_date, end_date, max_records=100):\n",
    "    \"\"\"Safe GDELT API request with error handling\"\"\"\n",
    "    url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    \n",
    "    params = {\n",
    "        'query': quote(query),  # URL-encode special characters\n",
    "        'format': 'json',\n",
    "        'startdatetime': start_date,\n",
    "        'enddatetime': end_date,\n",
    "        'mode': 'artlist',\n",
    "        'maxrecords': min(max_records, 250)  # API limit is 250\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Add timeout and custom headers\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "        }\n",
    "        response = requests.get(\n",
    "            url, \n",
    "            params=params, \n",
    "            headers=headers,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        # Check for empty response\n",
    "        if not response.content:\n",
    "            print(\"Empty API response\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        \n",
    "        # Check if articles exist\n",
    "        if not data.get('articles'):\n",
    "            print(\"No articles found\")\n",
    "            return None\n",
    "            \n",
    "        return data\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to decode JSON. Raw response: {response.text[:200]}...\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "data = get_gdelt_articles(\n",
    "    query='(crop disease OR plant pest) location:SriLanka',\n",
    "    start_date='20130101',\n",
    "    end_date='20231231',\n",
    "    max_records=100\n",
    ")\n",
    "\n",
    "if data:\n",
    "    for article in data['articles']:\n",
    "        print(f\"{article['date']} | {article['title']} | {article['url']}\")\n",
    "else:\n",
    "    print(\"No data received\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "28a2f3ea-2416-468f-9524-158fc694f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback to GDELT raw data files\n",
    "import pandas as pd\n",
    "\n",
    "def get_gdelt_raw_data(date):\n",
    "    \"\"\"Download raw GDELT data for specific date\"\"\"\n",
    "    url = f\"http://data.gdeltproject.org/gdeltv2/{date}.export.CSV.zip\"\n",
    "    try:\n",
    "        df = pd.read_csv(url, sep='\\t', header=None)\n",
    "        # Filter for Sri Lanka (column 53 contains country codes)\n",
    "        return df[df[53].str.contains('LKA', na=False)]\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {date}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d95125c8-a403-45f1-9768-18a188df01b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode JSON. Raw response: Invalid query start date format (must be in YYYYMMDDHHMMSS format).\n",
      "...\n",
      "No data received\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "\n",
    "def get_gdelt_articles(query, start_date, end_date, max_records=100):\n",
    "    \"\"\"Safe GDELT API request with error handling\"\"\"\n",
    "    url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    \n",
    "    params = {\n",
    "        'query': quote(query),  # URL-encode special characters\n",
    "        'format': 'json',\n",
    "        'startdatetime': start_date,\n",
    "        'enddatetime': end_date,\n",
    "        'mode': 'artlist',\n",
    "        'maxrecords': min(max_records, 250)  # API limit is 250\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Add timeout and custom headers\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "        }\n",
    "        response = requests.get(\n",
    "            url, \n",
    "            params=params, \n",
    "            headers=headers,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        # Check for empty response\n",
    "        if not response.content:\n",
    "            print(\"Empty API response\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        \n",
    "        # Check if articles exist\n",
    "        if not data.get('articles'):\n",
    "            print(\"No articles found\")\n",
    "            return None\n",
    "            \n",
    "        return data\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to decode JSON. Raw response: {response.text[:200]}...\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "data = get_gdelt_articles(\n",
    "    query='(crop disease OR plant pest) location:SriLanka',\n",
    "    start_date='20130101',\n",
    "    end_date='20231231',\n",
    "    max_records=100\n",
    ")\n",
    "\n",
    "if data:\n",
    "    for article in data['articles']:\n",
    "        print(f\"{article['date']} | {article['title']} | {article['url']}\")\n",
    "else:\n",
    "    print(\"No data received\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c641f3b-a268-4fbb-88b5-55cc66b8c201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7517290e-f54c-4e1e-b450-c80f93cedb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Expecting value: line 1 column 1 (char 0)\n",
      "API URL: https://api.gdeltproject.org/api/v2/doc/doc?query=%2528crop%2520disease%2520OR%2520plant%2520pest%2529%2520location%253ASriLanka&format=json&startdatetime=20130101000000&enddatetime=20231231235959&mode=artlist&maxrecords=50\n",
      "No results found. Try broadening your search.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "from datetime import datetime\n",
    "\n",
    "def get_gdelt_articles(query, start_date, end_date, max_records=100):\n",
    "    \"\"\"Safe GDELT API request with proper date formatting\"\"\"\n",
    "    url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    \n",
    "    # Convert dates to proper format (YYYYMMDD000000 if no time specified)\n",
    "    start_datetime = f\"{start_date}000000\" if len(start_date) == 8 else start_date\n",
    "    end_datetime = f\"{end_date}235959\" if len(end_date) == 8 else end_date\n",
    "    \n",
    "    params = {\n",
    "        'query': quote(query),\n",
    "        'format': 'json',\n",
    "        'startdatetime': start_datetime,\n",
    "        'enddatetime': end_datetime,\n",
    "        'mode': 'artlist',\n",
    "        'maxrecords': min(max_records, 250)\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url,\n",
    "            params=params,\n",
    "            headers={'User-Agent': 'Mozilla/5.0'},\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(f\"API URL: {response.url}\")  # Debug the actual request\n",
    "        return None\n",
    "\n",
    "# Example usage with proper dates\n",
    "data = get_gdelt_articles(\n",
    "    query='(crop disease OR plant pest) location:SriLanka',\n",
    "    start_date='20130101',  # Will be converted to 20130101000000\n",
    "    end_date='20231231',    # Will be converted to 20231231235959\n",
    "    max_records=50\n",
    ")\n",
    "\n",
    "if data and data.get('articles'):\n",
    "    for article in data['articles']:\n",
    "        print(f\"{article['date']} | {article['title']} | {article['url']}\")\n",
    "else:\n",
    "    print(\"No results found. Try broadening your search.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "94829a73-320e-435b-a600-009891a443fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trying query: 'crop disease Sri Lanka'\n",
      "API returned non-JSON response. Status: 200\n",
      "Response text: Invalid query start date.\n",
      "...\n",
      "No results. Trying next query...\n",
      "\n",
      "Trying query: 'plant pest Sri Lanka'\n",
      "API returned non-JSON response. Status: 200\n",
      "Response text: Invalid query start date.\n",
      "...\n",
      "No results. Trying next query...\n",
      "\n",
      "Trying query: 'agriculture disease Sri Lanka'\n",
      "API returned non-JSON response. Status: 200\n",
      "Response text: Invalid query start date.\n",
      "...\n",
      "No results. Trying next query...\n",
      "\n",
      "Trying query: 'Sri Lanka AND (blight OR pest OR disease)'\n",
      "API returned non-JSON response. Status: 200\n",
      "Response text: Invalid query start date.\n",
      "...\n",
      "No results. Trying next query...\n",
      "\n",
      "All queries failed. Try these alternatives:\n",
      "1. Use broader search terms\n",
      "2. Try a shorter date range\n",
      "3. Check https://blog.gdeltproject.org/api-debugging-tool/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def get_gdelt_articles(query, start_date, end_date, max_records=100):\n",
    "    \"\"\"Working GDELT API request with proper encoding\"\"\"\n",
    "    url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    \n",
    "    params = {\n",
    "        'query': query,  # Don't pre-encode the query\n",
    "        'format': 'json',\n",
    "        'startdatetime': f\"{start_date}000000\",\n",
    "        'enddatetime': f\"{end_date}235959\",\n",
    "        'mode': 'artlist',\n",
    "        'maxrecords': min(max_records, 250),\n",
    "        'sort': 'datedesc'  # Get newest articles first\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            url,\n",
    "            params=params,  # Let requests handle encoding\n",
    "            headers={'User-Agent': 'Mozilla/5.0'},\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        # Debug raw response if needed\n",
    "        if not response.text.strip():\n",
    "            print(\"Empty response from server\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        return data\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"API returned non-JSON response. Status: {response.status_code}\")\n",
    "        print(f\"Response text: {response.text[:200]}...\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Request failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Try these improved queries\n",
    "queries_to_try = [\n",
    "    'crop disease Sri Lanka',\n",
    "    'plant pest Sri Lanka',\n",
    "    'agriculture disease Sri Lanka',\n",
    "    'Sri Lanka AND (blight OR pest OR disease)'\n",
    "]\n",
    "\n",
    "for query in queries_to_try:\n",
    "    print(f\"\\nTrying query: '{query}'\")\n",
    "    data = get_gdelt_articles(\n",
    "        query=query,\n",
    "        start_date='20130101',\n",
    "        end_date='20231231',\n",
    "        max_records=50\n",
    "    )\n",
    "    \n",
    "    if data and data.get('articles'):\n",
    "        print(f\"Found {len(data['articles'])} articles:\")\n",
    "        for article in data['articles'][:3]:  # Show first 3 results\n",
    "            print(f\" - {article['date']}: {article['title']}\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"No results. Trying next query...\")\n",
    "\n",
    "if not data or not data.get('articles'):\n",
    "    print(\"\\nAll queries failed. Try these alternatives:\")\n",
    "    print(\"1. Use broader search terms\")\n",
    "    print(\"2. Try a shorter date range\")\n",
    "    print(\"3. Check https://blog.gdeltproject.org/api-debugging-tool/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf93769-e68d-4abd-b309-025bade58bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API error for 20130101-20130131: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20130131-20130302: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20130302-20130401: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20130401-20130501: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20130501-20130531: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20130531-20130630: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20130630-20130730: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20130730-20130829: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20130829-20130928: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20130928-20131028: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20131028-20131127: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20131127-20131227: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20131227-20140126: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20140126-20140225: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20140225-20140327: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20140327-20140426: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20140426-20140526: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20140526-20140625: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20140625-20140725: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20140725-20140824: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20140824-20140923: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20140923-20141023: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20141023-20141122: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20141122-20141222: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20141222-20150121: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20150121-20150220: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20150220-20150322: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20150322-20150421: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20150421-20150521: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20150521-20150620: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20150620-20150720: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20150720-20150819: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20150819-20150918: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20150918-20151018: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20151018-20151117: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20151117-20151217: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20151217-20160116: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20160116-20160215: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20160215-20160316: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20160316-20160415: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "API error for 20160415-20160515: Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def fetch_gdelt_articles():\n",
    "    base_url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    \n",
    "    # Break into smaller time periods to avoid API limits\n",
    "    date_ranges = []\n",
    "    current_date = datetime(2013, 1, 1)\n",
    "    end_date = datetime.now()\n",
    "    \n",
    "    while current_date < end_date:\n",
    "        next_date = current_date + timedelta(days=30)  # 1-month chunks\n",
    "        date_ranges.append((\n",
    "            current_date.strftime('%Y%m%d%H%M%S'),\n",
    "            min(next_date, end_date).strftime('%Y%m%d%H%M%S')\n",
    "        ))\n",
    "        current_date = next_date\n",
    "    \n",
    "    all_articles = []\n",
    "    \n",
    "    for start, end in date_ranges:\n",
    "        params = {\n",
    "            'query': 'sri lanka (crop OR plant) (disease OR pest OR blight)',\n",
    "            'format': 'json',\n",
    "            'startdatetime': start,\n",
    "            'enddatetime': end,\n",
    "            'mode': 'artlist',\n",
    "            'maxrecords': 100,\n",
    "            'sort': 'datedesc'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if 'articles' in data:\n",
    "                    all_articles.extend(data['articles'])\n",
    "                    print(f\"Found {len(data['articles'])} articles from {start[:8]} to {end[:8]}\")\n",
    "                else:\n",
    "                    print(f\"No articles in {start[:8]}-{end[:8]}\")\n",
    "            else:\n",
    "                print(f\"API error for {start[:8]}-{end[:8]}: {response.text[:100]}\")\n",
    "            \n",
    "            time.sleep(1)  # Be gentle with the API\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {start[:8]}-{end[:8]}: {str(e)}\")\n",
    "    \n",
    "    return all_articles\n",
    "\n",
    "# Run the collection\n",
    "articles = fetch_gdelt_articles()\n",
    "\n",
    "if articles:\n",
    "    df = pd.DataFrame(articles)[['date', 'title', 'url', 'source']]\n",
    "    print(f\"\\nTotal articles collected: {len(df)}\")\n",
    "    print(\"\\nSample results:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('sri_lanka_crop_diseases.csv', index=False)\n",
    "    print(\"\\nSaved to sri_lanka_crop_diseases.csv\")\n",
    "else:\n",
    "    print(\"No articles found. Try adjusting the query or date range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3e0679d5-bd1a-48a2-b2a8-2abcaaea687e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting GDELT API collection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fetching GDELT API Data:   0%|                                                                | 0/4528 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                                                                                                 \n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limited at 2013-01-01, switching to fallback...\n",
      "Error at 2013-01-01: 'tqdm_notebook' object has no attribute 'container'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tqdm_notebook' object has no attribute 'container'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mfetch_gdelt_api\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlimit requests\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response.text:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43mtqdm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRate limited at \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m, switching to fallback...\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:720\u001b[39m, in \u001b[36mtqdm.write\u001b[39m\u001b[34m(cls, s, file, end, nolock)\u001b[39m\n\u001b[32m    719\u001b[39m fp = file \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sys.stdout\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexternal_write_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnolock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnolock\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Write the message\u001b[39;49;00m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:148\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:750\u001b[39m, in \u001b[36mtqdm.external_write_mode\u001b[39m\u001b[34m(cls, file, nolock)\u001b[39m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m inst \u001b[38;5;129;01min\u001b[39;00m inst_cleared:\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m         \u001b[43minst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnolock\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:1347\u001b[39m, in \u001b[36mtqdm.refresh\u001b[39m\u001b[34m(self, nolock, lock_args)\u001b[39m\n\u001b[32m   1346\u001b[39m         \u001b[38;5;28mself\u001b[39m._lock.acquire()\n\u001b[32m-> \u001b[39m\u001b[32m1347\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\notebook.py:156\u001b[39m, in \u001b[36mtqdm_notebook.display\u001b[39m\u001b[34m(self, msg, pos, close, bar_style, check_delay)\u001b[39m\n\u001b[32m    154\u001b[39m     msg = \u001b[38;5;28mself\u001b[39m.format_meter(**d)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m ltext, pbar, rtext = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontainer\u001b[49m.children\n\u001b[32m    157\u001b[39m pbar.value = \u001b[38;5;28mself\u001b[39m.n\n",
      "\u001b[31mAttributeError\u001b[39m: 'tqdm_notebook' object has no attribute 'container'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# Try API first\u001b[39;00m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAttempting GDELT API collection...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     api_df = \u001b[43mfetch_gdelt_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m# Fallback to raw data if API fails\u001b[39;00m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m api_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(api_df) < \u001b[32m50\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mfetch_gdelt_api\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43mtqdm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mError at \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m current = next_date\n\u001b[32m     50\u001b[39m time.sleep(\u001b[32m6\u001b[39m)  \u001b[38;5;66;03m# 6 seconds between requests (above their 5s requirement)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:720\u001b[39m, in \u001b[36mtqdm.write\u001b[39m\u001b[34m(cls, s, file, end, nolock)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Print a message via tqdm (without overlap with bars).\"\"\"\u001b[39;00m\n\u001b[32m    719\u001b[39m fp = file \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sys.stdout\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexternal_write_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnolock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnolock\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Write the message\u001b[39;49;00m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:148\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:750\u001b[39m, in \u001b[36mtqdm.external_write_mode\u001b[39m\u001b[34m(cls, file, nolock)\u001b[39m\n\u001b[32m    748\u001b[39m     \u001b[38;5;66;03m# Force refresh display of bars we cleared\u001b[39;00m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m inst \u001b[38;5;129;01min\u001b[39;00m inst_cleared:\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m         \u001b[43minst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnolock\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:1347\u001b[39m, in \u001b[36mtqdm.refresh\u001b[39m\u001b[34m(self, nolock, lock_args)\u001b[39m\n\u001b[32m   1345\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1346\u001b[39m         \u001b[38;5;28mself\u001b[39m._lock.acquire()\n\u001b[32m-> \u001b[39m\u001b[32m1347\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[32m   1349\u001b[39m     \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\notebook.py:156\u001b[39m, in \u001b[36mtqdm_notebook.display\u001b[39m\u001b[34m(self, msg, pos, close, bar_style, check_delay)\u001b[39m\n\u001b[32m    152\u001b[39m     d[\u001b[33m'\u001b[39m\u001b[33mbar_format\u001b[39m\u001b[33m'\u001b[39m] = (d[\u001b[33m'\u001b[39m\u001b[33mbar_format\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{l_bar}\u001b[39;00m\u001b[33m<bar/>\u001b[39m\u001b[38;5;132;01m{r_bar}\u001b[39;00m\u001b[33m\"\u001b[39m).replace(\n\u001b[32m    153\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{bar}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m<bar/>\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    154\u001b[39m     msg = \u001b[38;5;28mself\u001b[39m.format_meter(**d)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m ltext, pbar, rtext = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontainer\u001b[49m.children\n\u001b[32m    157\u001b[39m pbar.value = \u001b[38;5;28mself\u001b[39m.n\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m msg:\n",
      "\u001b[31mAttributeError\u001b[39m: 'tqdm_notebook' object has no attribute 'container'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_FILE = \"sri_lanka_crop_diseases_2013_present.csv\"\n",
    "START_DATE = datetime(2013, 1, 1)\n",
    "END_DATE = datetime.now()\n",
    "\n",
    "## Phase 1: Polite API Requests (With Rate Limiting)\n",
    "def fetch_gdelt_api():\n",
    "    base_url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    collected = []\n",
    "    \n",
    "    # Process quarterly instead of monthly to reduce requests\n",
    "    current = START_DATE\n",
    "    pbar = tqdm(desc=\"Fetching GDELT API Data\", total=(END_DATE - START_DATE).days)\n",
    "    \n",
    "    while current < END_DATE:\n",
    "        next_date = min(current + timedelta(days=90), END_DATE)  # 3-month chunks\n",
    "        \n",
    "        params = {\n",
    "            'query': '(sri lanka OR lka) (crop OR plant) (disease OR pest OR blight)',\n",
    "            'format': 'json',\n",
    "            'startdatetime': current.strftime('%Y%m%d%H%M%S'),\n",
    "            'enddatetime': next_date.strftime('%Y%m%d%H%M%S'),\n",
    "            'mode': 'artlist',\n",
    "            'maxrecords': 100,\n",
    "            'sort': 'datedesc'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data.get('articles'):\n",
    "                    collected.extend(data['articles'])\n",
    "                    pbar.update((next_date - current).days)\n",
    "            elif \"limit requests\" in response.text:\n",
    "                tqdm.write(f\"Rate limited at {current.date()}, switching to fallback...\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Error at {current.date()}: {str(e)}\")\n",
    "            \n",
    "        current = next_date\n",
    "        time.sleep(6)  # 6 seconds between requests (above their 5s requirement)\n",
    "    \n",
    "    pbar.close()\n",
    "    return pd.DataFrame(collected)[['date', 'title', 'url', 'source']] if collected else None\n",
    "\n",
    "## Phase 2: Direct File Download Fallback\n",
    "def download_gdelt_raw():\n",
    "    base_url = \"http://data.gdeltproject.org/events/{year}.zip\"\n",
    "    collected = []\n",
    "    \n",
    "    for year in tqdm(range(START_DATE.year, END_DATE.year + 1), desc=\"Downloading Raw Data\"):\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                base_url.format(year=year),\n",
    "                sep='\\t', \n",
    "                header=None,\n",
    "                compression='zip',\n",
    "                encoding='latin1',\n",
    "                usecols=[1, 2, 26, 27, 53]  # Date, Actor1, EventCode, URL, Country\n",
    "            )\n",
    "            # Filter for Sri Lanka and agricultural events\n",
    "            filtered = df[(df[53] == 'LKA') & (df[26].astype(str).str.startswith('034'))]\n",
    "            collected.append(filtered)\n",
    "            time.sleep(2)  # Be gentle with their servers\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Error downloading {year}: {str(e)}\")\n",
    "    \n",
    "    if collected:\n",
    "        combined = pd.concat(collected)\n",
    "        combined.columns = ['date', 'actor', 'event_code', 'url', 'country']\n",
    "        return combined\n",
    "    return None\n",
    "\n",
    "## Phase 3: Local Sri Lankan Sources\n",
    "def scrape_local_sources():\n",
    "    sources = {\n",
    "        \"Agriculture Dept\": \"http://www.agridept.gov.lk\",\n",
    "        \"FAO Sri Lanka\": \"http://www.fao.org/sri-lanka/news/en/\",\n",
    "        \"Tea Research Inst\": \"http://www.tri.lk/publications/\"\n",
    "    }\n",
    "    \n",
    "    collected = []\n",
    "    # Implement scraping logic for each source\n",
    "    # (Would need custom selectors for each site)\n",
    "    \n",
    "    return pd.DataFrame(collected) if collected else None\n",
    "\n",
    "## Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Try API first\n",
    "    print(\"Attempting GDELT API collection...\")\n",
    "    api_df = fetch_gdelt_api()\n",
    "    \n",
    "    # Fallback to raw data if API fails\n",
    "    if api_df is None or len(api_df) < 50:\n",
    "        print(\"\\nAPI results insufficient, trying raw data download...\")\n",
    "        raw_df = download_gdelt_raw()\n",
    "    else:\n",
    "        raw_df = None\n",
    "    \n",
    "    # Combine results\n",
    "    final_df = pd.concat([df for df in [api_df, raw_df] if df is not None])\n",
    "    \n",
    "    if final_df is not None and len(final_df) > 0:\n",
    "        print(f\"\\nCollected {len(final_df)} records\")\n",
    "        final_df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"Saved to {OUTPUT_FILE}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"\\nSample results:\")\n",
    "        print(final_df.head())\n",
    "    else:\n",
    "        print(\"\\nFailed to collect data. Try:\")\n",
    "        print(\"1. Running again later (rate limits reset)\")\n",
    "        print(\"2. Using the direct download links manually\")\n",
    "        print(\"3. Contacting kalev.leetaru5@gmail.com for bulk access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eb437e37-842c-4501-a739-4758aa7f0700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tqdm_notebook (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for tqdm_notebook\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e767575-119d-4a2c-8275-28fc2bd787ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data collection...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd478fd83a69430b934113e4cc42ce9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting GDELT Data:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limited at 2013-01-01, slowing down...\n",
      "Rate limited at 2013-06-30, slowing down...\n",
      "Rate limited at 2013-12-27, slowing down...\n",
      "Rate limited at 2014-06-25, slowing down...\n",
      "Rate limited at 2014-12-22, slowing down...\n",
      "Rate limited at 2015-06-20, slowing down...\n",
      "Rate limited at 2015-12-17, slowing down...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting data collection...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     final_data = \u001b[43mfetch_gdelt_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     results = save_and_show_results(final_data)\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mfetch_gdelt_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlimit requests\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response.text:\n\u001b[32m     52\u001b[39m         tqdm.write(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRate limited at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date.date()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, slowing down...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait longer if rate limited\u001b[39;00m\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm  # Updated import for compatibility\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_FILE = \"sri_lanka_crop_diseases_final.csv\"\n",
    "START_DATE = datetime(2013, 1, 1)\n",
    "END_DATE = datetime.now()\n",
    "\n",
    "def fetch_gdelt_data():\n",
    "    \"\"\"Main function to collect crop disease data with proper rate limiting\"\"\"\n",
    "    base_url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "    collected_data = []\n",
    "    \n",
    "    # Calculate time chunks (6-month intervals to reduce requests)\n",
    "    date_chunks = []\n",
    "    current_date = START_DATE\n",
    "    while current_date < END_DATE:\n",
    "        next_date = min(current_date + timedelta(days=180), END_DATE)  # 6 months\n",
    "        date_chunks.append((current_date, next_date))\n",
    "        current_date = next_date\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    pbar = tqdm(date_chunks, desc=\"Collecting GDELT Data\")\n",
    "    \n",
    "    for start_date, end_date in pbar:\n",
    "        pbar.set_postfix({\n",
    "            'period': f\"{start_date.date()} to {end_date.date()}\",\n",
    "            'collected': len(collected_data)\n",
    "        })\n",
    "        \n",
    "        params = {\n",
    "            'query': 'sri lanka (crop disease OR plant pest OR agricultural blight)',\n",
    "            'format': 'json',\n",
    "            'startdatetime': start_date.strftime('%Y%m%d000000'),\n",
    "            'enddatetime': end_date.strftime('%Y%m%d235959'),\n",
    "            'mode': 'artlist',\n",
    "            'maxrecords': 100,\n",
    "            'sort': 'datedesc'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data.get('articles'):\n",
    "                    collected_data.extend(data['articles'])\n",
    "            elif \"limit requests\" in response.text:\n",
    "                tqdm.write(f\"Rate limited at {start_date.date()}, slowing down...\")\n",
    "                time.sleep(10)  # Wait longer if rate limited\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Error processing {start_date.date()}: {str(e)}\")\n",
    "        \n",
    "        time.sleep(6)  # 6 seconds between requests\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if collected_data:\n",
    "        df = pd.DataFrame(collected_data)\n",
    "        keep_columns = ['date', 'title', 'url', 'source', 'seendate']\n",
    "        available_columns = [col for col in keep_columns if col in df.columns]\n",
    "        return df[available_columns]\n",
    "    return None\n",
    "\n",
    "def save_and_show_results(df):\n",
    "    \"\"\"Save data and display summary\"\"\"\n",
    "    if df is not None and len(df) > 0:\n",
    "        # Clean date format\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Save to CSV\n",
    "        df.to_csv(OUTPUT_FILE, index=False)\n",
    "        print(f\"\\nSuccessfully saved {len(df)} records to {OUTPUT_FILE}\")\n",
    "        \n",
    "        # Show summary\n",
    "        print(\"\\nDate range covered:\", df['date'].min(), \"to\", df['date'].max())\n",
    "        print(\"\\nTop sources:\")\n",
    "        print(df['source'].value_counts().head(10))\n",
    "        \n",
    "        print(\"\\nSample results:\")\n",
    "        return df.head()\n",
    "    else:\n",
    "        print(\"No data collected. Try adjusting the query or date range.\")\n",
    "        return None\n",
    "\n",
    "# Run the collection\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting data collection...\")\n",
    "    final_data = fetch_gdelt_data()\n",
    "    results = save_and_show_results(final_data)\n",
    "    \n",
    "    if results is not None:\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204c179b-fecd-4f61-9e55-1e3af034d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm  # Progress bar for notebooks\n",
    "import langdetect \n",
    "from langdetect import detect\n",
    "\n",
    "#https://api.gdeltproject.org/api/v2/doc/doc?query=sri+lanka+agriculture&format=json&startdatetime=20230101000000&enddatetime=20230630235959&mode=artlist&maxrecords=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37c3e5c-a7a9-4cd7-b4b9-397c8e38904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_gdelt_articles(query, start_date, end_date, maxrecords=250):\n",
    "    \"\"\"\n",
    "    Fetch articles from GDELT API for a given query and date range.\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query (e.g., 'sri lanka agriculture').\n",
    "        start_date (datetime): Start datetime.\n",
    "        end_date (datetime): End datetime.\n",
    "        maxrecords (int): Max records to fetch (max 25000, usually 100+ is good).\n",
    "\n",
    "    Returns:\n",
    "        list of articles (dict)\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "        \n",
    "    params = {\n",
    "        'query': query,\n",
    "        'format': 'json',\n",
    "        'startdatetime': start_date.strftime('%Y%m%d%H%M%S'),\n",
    "        'enddatetime': end_date.strftime('%Y%m%d%H%M%S'),\n",
    "        'mode': 'artlist',\n",
    "        'maxrecords': maxrecords,\n",
    "        'sort': 'datedesc'\n",
    "    }\n",
    "\n",
    "    # Add proper headers\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    enriched_articles = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params,headers=headers, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                articles = data.get('articles', [])\n",
    "\n",
    "                for article in articles:\n",
    "                    article['sourcecountry'] = \"Sri Lanka\"\n",
    "                    article['language'] = \"English\"\n",
    "                    if 'title' in article and detect(article['title']) == 'en':\n",
    "                        enriched_articles.append(article)\n",
    "                return enriched_articles\n",
    "            \n",
    "        \n",
    "        elif response.status_code == 429:\n",
    "            print(\" Rate limited. Need to slow down.\")\n",
    "            return None\n",
    "        \n",
    "        else:\n",
    "            print(f\"Error: Received status code {response.status_code}\")\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Exception during API call: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd51e0db-5b29-4aed-b137-d462983ae323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_gdelt_data(query, start_date, end_date, chunk_days=180, delay_sec=6):\n",
    "    \"\"\"\n",
    "    Collect data in chunks of chunk_days with delay_sec seconds between requests.\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame with all articles collected.\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "    \n",
    "    current_start = start_date\n",
    "    \n",
    "    date_ranges = []\n",
    "    while current_start < end_date:\n",
    "        current_end = min(current_start + timedelta(days=chunk_days), end_date)\n",
    "        date_ranges.append((current_start, current_end))\n",
    "        current_start = current_end + timedelta(seconds=1)  # avoid overlap\n",
    "    \n",
    "    print(f\"Collecting data in {len(date_ranges)} chunks of {chunk_days} days each.\")\n",
    "    \n",
    "    for s, e in tqdm(date_ranges):\n",
    "        print(f\"\\nFetching data from {s.date()} to {e.date()}\")\n",
    "        \n",
    "        articles = fetch_gdelt_articles(query, s, e)\n",
    "        \n",
    "        if articles is None:\n",
    "            print(\"Sleeping 30 seconds due to rate limiting or error...\")\n",
    "            time.sleep(30)\n",
    "            # Retry once after wait\n",
    "            articles = fetch_gdelt_articles(query, s, e)\n",
    "            if articles is None:\n",
    "                print(\"Skipping this chunk due to repeated error.\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Received {len(articles)} articles.\")\n",
    "        all_articles.extend(articles)\n",
    "        \n",
    "        print(f\"Sleeping {delay_sec} seconds before next request...\")\n",
    "        time.sleep(delay_sec)\n",
    "    \n",
    "    if all_articles:\n",
    "        df = pd.DataFrame(all_articles)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No articles collected.\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa5578-bed5-4bbd-81c6-9ed8fcba7663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data in 38 chunks of 30 days each.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12abf417c7a04339b961eb5fa4e6261d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching data from 2020-06-01 to 2020-07-01\n",
      "Exception during API call: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping 30 seconds due to rate limiting or error...\n",
      "Exception during API call: Expecting value: line 1 column 1 (char 0)\n",
      "Skipping this chunk due to repeated error.\n",
      "\n",
      "Fetching data from 2020-07-01 to 2020-07-31\n",
      "Exception during API call: Expecting value: line 1 column 1 (char 0)\n",
      "Sleeping 30 seconds due to rate limiting or error...\n"
     ]
    }
   ],
   "source": [
    "# Define your query and date range here\n",
    "query = \"Sri+Lanka+Faces+Crop+Crisis\"\n",
    "start_date = datetime(2020, 6, 1)\n",
    "end_date = datetime(2023, 6, 30)\n",
    "\n",
    "# Collect data\n",
    "df = collect_gdelt_data(query,start_date, end_date, chunk_days=30, delay_sec=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83713d2e-309e-43e9-adf0-29b59e3e2786",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m.head(\u001b[32m150\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8762a135-6a6e-4d5e-a0cc-4935374e3827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected 3345 articles.\n",
      "         date                                              title  \\\n",
      "0  2020-07-02  15 Best Dry Shampoos for 2020 - Top Dry Shampo...   \n",
      "1  2020-07-01                                    California Cool   \n",
      "2  2020-07-01   The 15 Best New Beauty Products Dropping in July   \n",
      "3  2020-07-01  15 Best Dry Shampoos for 2020 - Top Dry Shampo...   \n",
      "4  2020-07-01                                         The Island   \n",
      "\n",
      "              domain sourcecountry  \n",
      "0           wtae.com     Sri Lanka  \n",
      "1  winespectator.com     Sri Lanka  \n",
      "2            msn.com     Sri Lanka  \n",
      "3           wjcl.com     Sri Lanka  \n",
      "4          island.lk     Sri Lanka  \n",
      "Saved to mine.csv\n"
     ]
    }
   ],
   "source": [
    "if not df.empty:\n",
    "    # Convert 'date' column to datetime and format\n",
    "    df['date'] = pd.to_datetime(df['seendate']).dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"\\nCollected {len(df)} articles.\")\n",
    "    print(df[['date', 'title', 'domain', 'sourcecountry']].head())\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(\"mine.csv\", index=False)\n",
    "    print(\"Saved to mine.csv\")\n",
    "else:\n",
    "    print(\"No data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf259ce2-650a-4ecc-8ee9-5e5130645c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(url):\n",
    "    \"\"\"Scrape full article content using newspaper3k\"\"\"\n",
    "    article = {\n",
    "        'url': url,\n",
    "        'title': '',\n",
    "        'text': '',\n",
    "        'publish_date': None,\n",
    "        'scrape_success': False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Using newspaper3k (best for news articles)\n",
    "        news_article = newspaper.Article(url)\n",
    "        news_article.download()\n",
    "        news_article.parse()\n",
    "        \n",
    "        article.update({\n",
    "            'title': news_article.title,\n",
    "            'text': news_article.text,\n",
    "            'publish_date': news_article.publish_date,\n",
    "            'scrape_success': True\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {str(e)}\")\n",
    "    \n",
    "    return article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197d50c-5577-4684-a05c-1997bf416acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import newspaper\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "# 3. Main function\n",
    "def get_sri_lanka_news(query, days_back=7, max_articles=5):\n",
    "    \"\"\"Full pipeline to get and scrape Sri Lankan news\"\"\"\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days_back)\n",
    "    \n",
    "    print(f\"Fetching URLs about '{query}' from {start_date.date()} to {end_date.date()}\")\n",
    "    urls = get_gdelt_urls(query, start_date, end_date, max_articles)\n",
    "    \n",
    "    if not urls:\n",
    "        print(\"No articles found\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(urls)} articles. Starting scraping...\")\n",
    "    \n",
    "    all_articles = []\n",
    "    for url in urls:\n",
    "        article = scrape_article(url)\n",
    "        if article['scrape_success']:\n",
    "            all_articles.append(article)\n",
    "        time.sleep(2)  # Be polite to websites\n",
    "        \n",
    "    print(f\"Successfully scraped {len(all_articles)}/{len(urls)} articles\")\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362169a-e2b0-44cc-8982-b84b05e4e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Get news about Sri Lankan agriculture\n",
    "    articles = get_sri_lanka_news(\n",
    "        query=\"agriculture OR farming\",\n",
    "        days_back=30,\n",
    "        max_articles=5\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    with open('sri_lanka_agriculture_news.json', 'w') as f:\n",
    "        json.dump(articles, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Saved {len(articles)} articles to JSON file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e069a89-a5af-4119-b3e8-fb877026c2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be68489-6be9-47bb-828d-ce714846a578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "927049fe-e4a1-4dfa-a998-bfaa96ca7fd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"url\": \"https://www.ft.lk/business/EDB-and-New-Zealand-explore-agri-exports-boost/34-776891\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"EDB and New Zealand explore agri - exports boost\",\n",
      "    \"seendate\": \"20250526T010000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"ft.lk\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.ft.lk/ft_view__editorial/Harnessing-Sri-Lanka-s-biodiversity-for-national-benefit/58-776864\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"Harnessing Sri Lanka biodiversity for national benefit\",\n",
      "    \"seendate\": \"20250526T010000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"ft.lk\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.ft.lk/front-page/NEXT-closure-exposes-antics-of-self-centred-trade-unionism/44-776902\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"NEXT closure exposes antics of self - centred trade unionism\",\n",
      "    \"seendate\": \"20250526T010000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"ft.lk\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://hea.china.com/articles/20250526/202505261677748.html\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"20+\\u57ce 40\\u9662\\uff5c\\u9f0e\\u690d\\u53e3\\u8154 \\uff1a \\u7814\\u7591\\u96be\\u6280\\u672f \\uff0c \\u62d3\\u8bda\\u4fe1\\u7586\\u57df _ \\u4e2d\\u534e\\u7f51\",\n",
      "    \"seendate\": \"20250526T113000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"hea.china.com\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.thehindu.com/society/indias-debut-global-botanical-art-exhibition-featured-some-stunning-works-from-the-country-and-abroad/article69605605.ece\",\n",
      "    \"url_mobile\": \"https://www.thehindu.com/society/indias-debut-global-botanical-art-exhibition-featured-some-stunning-works-from-the-country-and-abroad/article69605605.ece/amp/\",\n",
      "    \"title\": \"India debut Global Botanical Art Exhibition featured some stunning works from the country and abroad\",\n",
      "    \"seendate\": \"20250526T110000Z\",\n",
      "    \"socialimage\": \"https://th-i.thgim.com/public/life-and-style/g7az6f/article69609450.ece/alternates/LANDSCAPE_1200/24mpLenin.jpg\",\n",
      "    \"domain\": \"thehindu.com\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.vetogate.com/5418342\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u0641\\u0648\\u0627\\u0626\\u062f \\u0627\\u0644\\u0642\\u0631\\u0646\\u0641\\u0644 \\u060c \\u064a\\u0637\\u0631\\u062f \\u062f\\u064a\\u062f\\u0627\\u0646 \\u0627\\u0644\\u0645\\u0639\\u062f\\u0629 \\u0648\\u064a\\u0633\\u0643\\u0646 \\u0627\\u0644\\u0622\\u0644\\u0627\\u0645 \\u0648\\u064a\\u0642\\u0648\\u0649 \\u0627\\u0644\\u0645\\u0646\\u0627\\u0639\\u0629\",\n",
      "    \"seendate\": \"20250526T091500Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"vetogate.com\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://finance.eastmoney.com/a/202505263413889917.html\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u4e00\\u5468\\u524d\\u77bb | \\u7f8e\\u8054\\u50a8\\u6700\\u7231\\u901a\\u80c0\\u6570\\u636e\\u6765\\u88ad \\uff1b \\u82f1\\u4f1f\\u8fbe \\u3001 \\u5c0f\\u7c73 \\u3001 \\u62fc\\u591a\\u591a\\u5c06\\u653e\\u699c \\uff1b \\u5468\\u4e00\\u7f8e\\u80a1\\u4f11\\u5e02 _ \\u4e1c\\u65b9\\u8d22\\u5bcc\\u7f51\",\n",
      "    \"seendate\": \"20250526T024500Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"finance.eastmoney.com\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://baijiahao.baidu.com/s?id=1833089146217297314\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u53e4\\u90fd\\u643a\\u624b\\u4e1d\\u8def \\uff01 \\u4e1d\\u535a\\u4f1a\\u7efd\\u653e\\u65b0\\u6d3b\\u529b\",\n",
      "    \"seendate\": \"20250526T104500Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"baijiahao.baidu.com\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://baijiahao.baidu.com/s?id=1833142461575631605\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u5370\\u5ea6\\u624b\\u673a\\u5236\\u9020\\u4e1a\\u96c4\\u5fc3 \\uff0c \\u906d  \\u7f8e\\u56fd\\u4f18\\u5148  \\u91cd\\u51fb\",\n",
      "    \"seendate\": \"20250526T023000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"baijiahao.baidu.com\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://news.fudan.edu.cn/2025/0521/c1268a145455/page.htm\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u4f4e\\u98ce\\u9669\\u5352\\u4e2d\\u60a3\\u8005\\u76d1\\u6d4b\\u9891\\u7387\\u53ef\\u51cf\\u534a \\uff01 \\u590d\\u65e6\\u9886\\u8854\\u8de8\\u56fd\\u7814\\u7a76\\u767b \\u300a \\u67f3\\u53f6\\u5200 \\u300b \",\n",
      "    \"seendate\": \"20250526T113000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"news.fudan.edu.cn\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "\n",
    "# Properly encoded parameters\n",
    "params = {\n",
    "    'query': '(crop disease OR plant pest OR agricultural blight) AND sri lanka',\n",
    "    'format': 'json',\n",
    "    'startdatetime': today.strftime('%Y%m%d000000'),\n",
    "    'enddatetime': today.strftime('%Y%m%d235959'),\n",
    "    'mode': 'artlist',\n",
    "    'maxrecords': '10'  # String instead of integer\n",
    "}\n",
    "\n",
    "# Add proper headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "    \n",
    "    # Check if response is valid JSON\n",
    "    if response.text.strip():\n",
    "        data = response.json()\n",
    "        if 'articles' in data:\n",
    "    # Add sourcecountry and language fields to each article\n",
    "            enriched_articles = []\n",
    "            for article in data['articles']:\n",
    "                article['sourcecountry'] = \"Sri Lanka\"\n",
    "                article['language'] = \"English\"\n",
    "                enriched_articles.append(article)\n",
    "    \n",
    "            print(json.dumps(enriched_articles, indent=2))\n",
    "      \n",
    "        else:\n",
    "            print(\"Unexpected response format:\", data)\n",
    "    else:\n",
    "        print(\"Empty response from server. Status code:\", response.status_code)\n",
    "        print(\"Response text:\", response.text)\n",
    "        \n",
    "except json.JSONDecodeError:\n",
    "    print(\"Failed to decode JSON. Raw response:\")\n",
    "    print(response.text)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"Request failed:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acb88ca9-0348-4d8f-9f87-59cfda227a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"url\": \"https://www.ft.lk/business/EDB-and-New-Zealand-explore-agri-exports-boost/34-776891\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"EDB and New Zealand explore agri - exports boost\",\n",
      "    \"seendate\": \"20250526T010000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"ft.lk\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.ft.lk/ft_view__editorial/Harnessing-Sri-Lanka-s-biodiversity-for-national-benefit/58-776864\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"Harnessing Sri Lanka biodiversity for national benefit\",\n",
      "    \"seendate\": \"20250526T010000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"ft.lk\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.ft.lk/front-page/NEXT-closure-exposes-antics-of-self-centred-trade-unionism/44-776902\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"NEXT closure exposes antics of self - centred trade unionism\",\n",
      "    \"seendate\": \"20250526T010000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"ft.lk\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://hea.china.com/articles/20250526/202505261677748.html\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"20+\\u57ce 40\\u9662\\uff5c\\u9f0e\\u690d\\u53e3\\u8154 \\uff1a \\u7814\\u7591\\u96be\\u6280\\u672f \\uff0c \\u62d3\\u8bda\\u4fe1\\u7586\\u57df _ \\u4e2d\\u534e\\u7f51\",\n",
      "    \"seendate\": \"20250526T113000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"hea.china.com\",\n",
      "    \"language\": \"Chinese\",\n",
      "    \"sourcecountry\": \"China\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.thehindu.com/society/indias-debut-global-botanical-art-exhibition-featured-some-stunning-works-from-the-country-and-abroad/article69605605.ece\",\n",
      "    \"url_mobile\": \"https://www.thehindu.com/society/indias-debut-global-botanical-art-exhibition-featured-some-stunning-works-from-the-country-and-abroad/article69605605.ece/amp/\",\n",
      "    \"title\": \"India debut Global Botanical Art Exhibition featured some stunning works from the country and abroad\",\n",
      "    \"seendate\": \"20250526T110000Z\",\n",
      "    \"socialimage\": \"https://th-i.thgim.com/public/life-and-style/g7az6f/article69609450.ece/alternates/LANDSCAPE_1200/24mpLenin.jpg\",\n",
      "    \"domain\": \"thehindu.com\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"India\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.vetogate.com/5418342\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u0641\\u0648\\u0627\\u0626\\u062f \\u0627\\u0644\\u0642\\u0631\\u0646\\u0641\\u0644 \\u060c \\u064a\\u0637\\u0631\\u062f \\u062f\\u064a\\u062f\\u0627\\u0646 \\u0627\\u0644\\u0645\\u0639\\u062f\\u0629 \\u0648\\u064a\\u0633\\u0643\\u0646 \\u0627\\u0644\\u0622\\u0644\\u0627\\u0645 \\u0648\\u064a\\u0642\\u0648\\u0649 \\u0627\\u0644\\u0645\\u0646\\u0627\\u0639\\u0629\",\n",
      "    \"seendate\": \"20250526T091500Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"vetogate.com\",\n",
      "    \"language\": \"Arabic\",\n",
      "    \"sourcecountry\": \"Egypt\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://finance.eastmoney.com/a/202505263413889917.html\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u4e00\\u5468\\u524d\\u77bb | \\u7f8e\\u8054\\u50a8\\u6700\\u7231\\u901a\\u80c0\\u6570\\u636e\\u6765\\u88ad \\uff1b \\u82f1\\u4f1f\\u8fbe \\u3001 \\u5c0f\\u7c73 \\u3001 \\u62fc\\u591a\\u591a\\u5c06\\u653e\\u699c \\uff1b \\u5468\\u4e00\\u7f8e\\u80a1\\u4f11\\u5e02 _ \\u4e1c\\u65b9\\u8d22\\u5bcc\\u7f51\",\n",
      "    \"seendate\": \"20250526T024500Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"finance.eastmoney.com\",\n",
      "    \"language\": \"Chinese\",\n",
      "    \"sourcecountry\": \"China\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://baijiahao.baidu.com/s?id=1833089146217297314\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u53e4\\u90fd\\u643a\\u624b\\u4e1d\\u8def \\uff01 \\u4e1d\\u535a\\u4f1a\\u7efd\\u653e\\u65b0\\u6d3b\\u529b\",\n",
      "    \"seendate\": \"20250526T104500Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"baijiahao.baidu.com\",\n",
      "    \"language\": \"Chinese\",\n",
      "    \"sourcecountry\": \"China\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://baijiahao.baidu.com/s?id=1833142461575631605\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u5370\\u5ea6\\u624b\\u673a\\u5236\\u9020\\u4e1a\\u96c4\\u5fc3 \\uff0c \\u906d  \\u7f8e\\u56fd\\u4f18\\u5148  \\u91cd\\u51fb\",\n",
      "    \"seendate\": \"20250526T023000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"baijiahao.baidu.com\",\n",
      "    \"language\": \"Chinese\",\n",
      "    \"sourcecountry\": \"China\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://news.fudan.edu.cn/2025/0521/c1268a145455/page.htm\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u4f4e\\u98ce\\u9669\\u5352\\u4e2d\\u60a3\\u8005\\u76d1\\u6d4b\\u9891\\u7387\\u53ef\\u51cf\\u534a \\uff01 \\u590d\\u65e6\\u9886\\u8854\\u8de8\\u56fd\\u7814\\u7a76\\u767b \\u300a \\u67f3\\u53f6\\u5200 \\u300b \",\n",
      "    \"seendate\": \"20250526T113000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"news.fudan.edu.cn\",\n",
      "    \"language\": \"Chinese\",\n",
      "    \"sourcecountry\": \"China\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "if 'articles' in data:\n",
    "    print(json.dumps(data['articles'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e844ecf0-54bf-4397-94e6-e5685dcd73a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"url\": \"https://www.ft.lk/business/EDB-and-New-Zealand-explore-agri-exports-boost/34-776891\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"EDB and New Zealand explore agri - exports boost\",\n",
      "    \"seendate\": \"20250526T010000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"ft.lk\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.ft.lk/ft_view__editorial/Harnessing-Sri-Lanka-s-biodiversity-for-national-benefit/58-776864\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"Harnessing Sri Lanka biodiversity for national benefit\",\n",
      "    \"seendate\": \"20250526T010000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"ft.lk\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.ft.lk/front-page/NEXT-closure-exposes-antics-of-self-centred-trade-unionism/44-776902\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"NEXT closure exposes antics of self - centred trade unionism\",\n",
      "    \"seendate\": \"20250526T010000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"ft.lk\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://hea.china.com/articles/20250526/202505261677748.html\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"20+\\u57ce 40\\u9662\\uff5c\\u9f0e\\u690d\\u53e3\\u8154 \\uff1a \\u7814\\u7591\\u96be\\u6280\\u672f \\uff0c \\u62d3\\u8bda\\u4fe1\\u7586\\u57df _ \\u4e2d\\u534e\\u7f51\",\n",
      "    \"seendate\": \"20250526T113000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"hea.china.com\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.thehindu.com/society/indias-debut-global-botanical-art-exhibition-featured-some-stunning-works-from-the-country-and-abroad/article69605605.ece\",\n",
      "    \"url_mobile\": \"https://www.thehindu.com/society/indias-debut-global-botanical-art-exhibition-featured-some-stunning-works-from-the-country-and-abroad/article69605605.ece/amp/\",\n",
      "    \"title\": \"India debut Global Botanical Art Exhibition featured some stunning works from the country and abroad\",\n",
      "    \"seendate\": \"20250526T110000Z\",\n",
      "    \"socialimage\": \"https://th-i.thgim.com/public/life-and-style/g7az6f/article69609450.ece/alternates/LANDSCAPE_1200/24mpLenin.jpg\",\n",
      "    \"domain\": \"thehindu.com\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.vetogate.com/5418342\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u0641\\u0648\\u0627\\u0626\\u062f \\u0627\\u0644\\u0642\\u0631\\u0646\\u0641\\u0644 \\u060c \\u064a\\u0637\\u0631\\u062f \\u062f\\u064a\\u062f\\u0627\\u0646 \\u0627\\u0644\\u0645\\u0639\\u062f\\u0629 \\u0648\\u064a\\u0633\\u0643\\u0646 \\u0627\\u0644\\u0622\\u0644\\u0627\\u0645 \\u0648\\u064a\\u0642\\u0648\\u0649 \\u0627\\u0644\\u0645\\u0646\\u0627\\u0639\\u0629\",\n",
      "    \"seendate\": \"20250526T091500Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"vetogate.com\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://finance.eastmoney.com/a/202505263413889917.html\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u4e00\\u5468\\u524d\\u77bb | \\u7f8e\\u8054\\u50a8\\u6700\\u7231\\u901a\\u80c0\\u6570\\u636e\\u6765\\u88ad \\uff1b \\u82f1\\u4f1f\\u8fbe \\u3001 \\u5c0f\\u7c73 \\u3001 \\u62fc\\u591a\\u591a\\u5c06\\u653e\\u699c \\uff1b \\u5468\\u4e00\\u7f8e\\u80a1\\u4f11\\u5e02 _ \\u4e1c\\u65b9\\u8d22\\u5bcc\\u7f51\",\n",
      "    \"seendate\": \"20250526T024500Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"finance.eastmoney.com\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://baijiahao.baidu.com/s?id=1833089146217297314\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u53e4\\u90fd\\u643a\\u624b\\u4e1d\\u8def \\uff01 \\u4e1d\\u535a\\u4f1a\\u7efd\\u653e\\u65b0\\u6d3b\\u529b\",\n",
      "    \"seendate\": \"20250526T104500Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"baijiahao.baidu.com\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://baijiahao.baidu.com/s?id=1833142461575631605\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u5370\\u5ea6\\u624b\\u673a\\u5236\\u9020\\u4e1a\\u96c4\\u5fc3 \\uff0c \\u906d  \\u7f8e\\u56fd\\u4f18\\u5148  \\u91cd\\u51fb\",\n",
      "    \"seendate\": \"20250526T023000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"baijiahao.baidu.com\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://news.fudan.edu.cn/2025/0521/c1268a145455/page.htm\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"\\u4f4e\\u98ce\\u9669\\u5352\\u4e2d\\u60a3\\u8005\\u76d1\\u6d4b\\u9891\\u7387\\u53ef\\u51cf\\u534a \\uff01 \\u590d\\u65e6\\u9886\\u8854\\u8de8\\u56fd\\u7814\\u7a76\\u767b \\u300a \\u67f3\\u53f6\\u5200 \\u300b \",\n",
      "    \"seendate\": \"20250526T113000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"news.fudan.edu.cn\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "if 'articles' in data:\n",
    "    # Add sourcecountry and language fields to each article\n",
    "    enriched_articles = []\n",
    "    for article in data['articles']:\n",
    "        article['sourcecountry'] = \"Sri Lanka\"\n",
    "        article['language'] = \"English\"\n",
    "        enriched_articles.append(article)\n",
    "    \n",
    "    print(json.dumps(enriched_articles, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15fd6c0-84b9-4bd6-a231-1836b0fd5a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\The User\\AppData\\Local\\Temp\\ipykernel_8512\\1169216275.py:26: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  today = datetime.utcnow() - timedelta(days=days_back)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Collected 4 articles (Date: 2025-05-26)\n",
      "\n",
      " Done! Collected 3 enriched articles.\n",
      "[\n",
      "  {\n",
      "    \"url\": \"https://www.ft.lk/business/EDB-and-New-Zealand-explore-agri-exports-boost/34-776891\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"EDB and New Zealand explore agri - exports boost\",\n",
      "    \"seendate\": \"20250526T010000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"ft.lk\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.ft.lk/ft_view__editorial/Harnessing-Sri-Lanka-s-biodiversity-for-national-benefit/58-776864\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"Harnessing Sri Lanka biodiversity for national benefit\",\n",
      "    \"seendate\": \"20250526T010000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"ft.lk\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.ft.lk/front-page/NEXT-closure-exposes-antics-of-self-centred-trade-unionism/44-776902\",\n",
      "    \"url_mobile\": \"\",\n",
      "    \"title\": \"NEXT closure exposes antics of self - centred trade unionism\",\n",
      "    \"seendate\": \"20250526T010000Z\",\n",
      "    \"socialimage\": \"\",\n",
      "    \"domain\": \"ft.lk\",\n",
      "    \"language\": \"English\",\n",
      "    \"sourcecountry\": \"Sri Lanka\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import html\n",
    "import langdetect  # Install with: pip install langdetect\n",
    "\n",
    "# Filter only English articles\n",
    "from langdetect import detect\n",
    "\n",
    "# Setup\n",
    "n = 3 # Target number of articles\n",
    "enriched_articles = []\n",
    "maxrecords = 10  # GDELT API allows up to 250, but use 10 for lighter requests\n",
    "days_back = 0  # Start from today\n",
    "\n",
    "# Headers and endpoint\n",
    "url = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "while len(enriched_articles) < n:\n",
    "    # Define date range (1-day window, going backward)\n",
    "    today = datetime.utcnow() - timedelta(days=days_back)\n",
    "    start_date = today.replace(hour=0, minute=0, second=0)\n",
    "    end_date = today.replace(hour=23, minute=59, second=59)\n",
    "\n",
    "    params = {\n",
    "        'query': '(crop disease OR plant pest OR agricultural blight) AND sri lanka',\n",
    "        'format': 'json',\n",
    "        'startdatetime': start_date.strftime('%Y%m%d%H%M%S'),\n",
    "        'enddatetime': end_date.strftime('%Y%m%d%H%M%S'),\n",
    "        'mode': 'artlist',\n",
    "        'maxrecords': str(maxrecords)\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "        if response.text.strip():\n",
    "            data = response.json()\n",
    "            articles = data.get('articles', [])\n",
    "\n",
    "            for article in articles:\n",
    "                article['sourcecountry'] = \"Sri Lanka\"\n",
    "                article['language'] = \"English\"\n",
    "                if 'title' in article and detect(article['title']) == 'en':\n",
    "                    enriched_articles.append(article)\n",
    "                    \n",
    "            print(f\" Collected {len(enriched_articles)} articles (Date: {today.date()})\")\n",
    "\n",
    "        else:\n",
    "            print(f\" Empty response on {today.date()}: Status {response.status_code}\")\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\" JSON decode error on {today.date()}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\" Request failed: {e}\")\n",
    "\n",
    "    days_back += 1  # Go further back in time\n",
    "    time.sleep(6)  # Respect API rate limit\n",
    "\n",
    "# Trim to exactly `n`\n",
    "enriched_articles = enriched_articles[:n]\n",
    "print(f\"\\n Done! Collected {len(enriched_articles)} enriched articles.\")\n",
    "\n",
    "# Optional: Print first 2 articles\n",
    "print(json.dumps(enriched_articles[:10], indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "090dc14d-17ed-4a49-8999-5d688709d932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 981.5/981.5 kB 4.2 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\the user\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langdetect) (1.17.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (pyproject.toml): started\n",
      "  Building wheel for langdetect (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993363 sha256=ca9d5abeacf11994b1eb43a79424df5c28abd6a35b6df96c4dcb60645df19c51\n",
      "  Stored in directory: c:\\users\\the user\\appdata\\local\\pip\\cache\\wheels\\eb\\87\\25\\2dddf1c94e1786054e25022ec5530bfed52bad86d882999c48\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ba129-dcbc-4980-8bda-3e27ed1c34f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
